{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045a2c8f-f704-4970-8601-7288e0927220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmasciol/miniconda3/envs/test_env/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, TrainingArguments,AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import random\n",
    "import peft\n",
    "from trl import SFTTrainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c00d6c7-eb35-4bfb-8279-2ccc59006b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdieplstks\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0e93e48-2e2c-4ec0-9107-d77765b59f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/accounts/eecs595f23_class_root/eecs595f23_class/cmasciol/wandb/run-20231212_160402-5k39klsj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dieplstks/BLIP-finetune/runs/5k39klsj' target=\"_blank\">dark-meadow-3</a></strong> to <a href='https://wandb.ai/dieplstks/BLIP-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dieplstks/BLIP-finetune' target=\"_blank\">https://wandb.ai/dieplstks/BLIP-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dieplstks/BLIP-finetune/runs/5k39klsj' target=\"_blank\">https://wandb.ai/dieplstks/BLIP-finetune/runs/5k39klsj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dieplstks/BLIP-finetune/runs/5k39klsj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14d61463ebd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"BLIP-finetune\",\n",
    "    config={\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"modules_to_save\": 'q-former',\n",
    "\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d86a218-0849-400a-8d66-4be1e090c56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594a571de9bb4314b8f07faedca8962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# by default `from_pretrained` loads the weights in float32\n",
    "# we load in float16 instead to save memory\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2ae8c7-4167-46be-a9f1-23ddd3fab4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191dfeb5-3ad7-449b-af64-2f92608301c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f022c6-b1e5-4c54-af1a-4e30904eef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6a779c-6195-4661-9d7e-eed681d80ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 105137664 || all params: 3744679936 || trainable%: 2.81\n"
     ]
    }
   ],
   "source": [
    "# model_clone = copy.deepcopy(model)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9518ae2b-30dd-4051-882c-b80524f2f9fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"qformer\"],\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1417d12-bfa0-40c6-806e-75a96ae89e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 110380544 || all params: 3855060480 || trainable%: 2.86\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66f8bd2-cc5a-4162-a6ea-bf3a46277066",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5757cf-14f9-419a-99f9-c41474e02ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.qformer.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a824b22-c982-4957-a4af-20a4dc4f2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(qid, data):\n",
    "    image = data[qid]['imageId']\n",
    "    question = data[qid]['question']\n",
    "    answer = data[qid]['answer']\n",
    "    full_answer = data[qid]['fullAnswer']\n",
    "    return image, question, answer, full_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3050bb90-b6b7-45c1-a2fa-554debd65644",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('questions/train_balanced_questions.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30998074-fc03-409c-a53c-7eaa60c6044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_keys = random.sample(list(data.keys()), 20000)\n",
    "train_data = {key: data[key] for key in random_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b2590b6-449c-49fd-bae0-88da20c6e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_questions_finetune.json\", 'w') as f:\n",
    "    json.dump(train_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dca2723-74c0-44a1-96ec-d7242df40477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moslty from: https://github.com/huggingface/notebooks/blob/main/peft/Fine_tune_BLIP2_on_an_image_captioning_dataset_PEFT.ipynb\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"question\"], \n",
    "                                  padding=\"max_length\", max_length=60, return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        # encoding[\"question\"] = item[\"question\"]\n",
    "        encoding[\"answer\"] = item[\"answer\"]\n",
    "        return encoding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # pad the input_ids and attention_mask\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key  not in [\"answer\"]:\n",
    "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "        else:\n",
    "            text_inputs = processor.tokenizer(\n",
    "                [example[\"answer\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "    return processed_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54d4bda3-13b2-4293-8e83-3a46735272ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for k in train_data:\n",
    "    image, question, answer, _ = get_data(k, train_data)\n",
    "    image = Image.open('images/'+image+'.jpg')\n",
    "    question_formatted = f'Answer the following question with one word. Question: {question} Answer:' \n",
    "    dataset.append({'image': image, 'question': question_formatted, 'answer': answer})\n",
    "\n",
    "\n",
    "# Create an instance of ImageCaptioningDataset\n",
    "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=100, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "842f2739-8a22-4603-bcc6-0957c10c7f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "668156ba-a98a-47dc-be60-ad536fd26be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "10 Loss: 11.2734375\n",
      "20 Loss: 11.3515625\n",
      "30 Loss: 10.8671875\n",
      "40 Loss: 11.421875\n",
      "50 Loss: 11.1875\n",
      "60 Loss: 11.5546875\n",
      "70 Loss: 11.2578125\n",
      "80 Loss: 11.3671875\n",
      "90 Loss: 10.6796875\n",
      "100 Loss: 11.34375\n",
      "110 Loss: 11.359375\n",
      "120 Loss: 11.34375\n",
      "130 Loss: 11.2421875\n",
      "140 Loss: 11.53125\n",
      "150 Loss: 11.5859375\n",
      "160 Loss: 11.1796875\n",
      "170 Loss: 11.2578125\n",
      "180 Loss: 10.75\n",
      "190 Loss: 10.875\n",
      "200 Loss: 11.390625\n",
      "Epoch: 1\n",
      "10 Loss: 11.1015625\n",
      "20 Loss: 10.6640625\n",
      "30 Loss: 11.1953125\n",
      "40 Loss: 10.7890625\n",
      "50 Loss: 11.0390625\n",
      "60 Loss: 10.6171875\n",
      "70 Loss: 11.0390625\n",
      "80 Loss: 10.734375\n",
      "90 Loss: 11.1796875\n",
      "100 Loss: 11.2734375\n",
      "110 Loss: 10.59375\n",
      "120 Loss: 11.0546875\n",
      "130 Loss: 10.8515625\n",
      "140 Loss: 11.046875\n",
      "150 Loss: 11.5234375\n",
      "160 Loss: 10.6328125\n",
      "170 Loss: 10.84375\n",
      "180 Loss: 11.2265625\n",
      "190 Loss: 10.53125\n",
      "200 Loss: 10.8671875\n",
      "Epoch: 2\n",
      "10 Loss: 11.296875\n",
      "20 Loss: 11.078125\n",
      "30 Loss: 10.6796875\n",
      "40 Loss: 10.3125\n",
      "50 Loss: 10.625\n",
      "60 Loss: 11.0234375\n",
      "70 Loss: 10.9140625\n",
      "80 Loss: 10.2734375\n",
      "90 Loss: 11.015625\n",
      "100 Loss: 10.046875\n",
      "110 Loss: 10.65625\n",
      "120 Loss: 10.6171875\n",
      "130 Loss: 11.0\n",
      "140 Loss: 10.53125\n",
      "150 Loss: 10.8359375\n",
      "160 Loss: 10.03125\n",
      "170 Loss: 10.296875\n",
      "180 Loss: 10.7578125\n",
      "190 Loss: 10.7578125\n",
      "200 Loss: 10.5859375\n",
      "Epoch: 3\n",
      "10 Loss: 9.890625\n",
      "20 Loss: 9.875\n",
      "30 Loss: 10.7265625\n",
      "40 Loss: 10.7109375\n",
      "50 Loss: 10.625\n",
      "60 Loss: 9.65625\n",
      "70 Loss: 10.3671875\n",
      "80 Loss: 10.4765625\n",
      "90 Loss: 9.7734375\n",
      "100 Loss: 9.9296875\n",
      "110 Loss: 9.734375\n",
      "120 Loss: 10.59375\n",
      "130 Loss: 10.421875\n",
      "140 Loss: 10.390625\n",
      "150 Loss: 9.8515625\n",
      "160 Loss: 10.53125\n",
      "170 Loss: 9.578125\n",
      "180 Loss: 10.0\n",
      "190 Loss: 9.390625\n",
      "200 Loss: 10.359375\n",
      "Epoch: 4\n",
      "10 Loss: 9.6953125\n",
      "20 Loss: 10.2265625\n",
      "30 Loss: 9.984375\n",
      "40 Loss: 9.1953125\n",
      "50 Loss: 10.0\n",
      "60 Loss: 9.4375\n",
      "70 Loss: 9.390625\n",
      "80 Loss: 9.890625\n",
      "90 Loss: 9.7734375\n",
      "100 Loss: 9.1015625\n",
      "110 Loss: 9.3828125\n",
      "120 Loss: 9.1171875\n",
      "130 Loss: 9.984375\n",
      "140 Loss: 9.859375\n",
      "150 Loss: 9.84375\n",
      "160 Loss: 9.140625\n",
      "170 Loss: 9.8125\n",
      "180 Loss: 9.84375\n",
      "190 Loss: 9.078125\n",
      "200 Loss: 9.765625\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(peft_model.parameters(), lr=5e-4)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "peft_model.train()\n",
    "for epoch in range(5):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    n = 0\n",
    "    for idx, batch in enumerate(train_dataloader):        \n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float32)\n",
    "        outputs = peft_model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        labels=input_ids)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        wandb.log({'training_loss': loss.item()})\n",
    "        \n",
    "        if (n+1) % 10 == 0:\n",
    "            print(f'{n+1} Loss: {loss.item()}')\n",
    "        n += 1\n",
    "        loss.backward()\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(peft_model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ada5aae5-8655-405a-8c23-12138fca4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(peft_model.state_dict(), 'finetuneqformer15epochstatedict.torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a095f6cd-2906-4ce5-ac73-b69c64224061",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(peft_model, 'finetuneqformer15epoch.torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f850b08-1995-467f-a500-d1516539682d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>▆▇▇▇█▇▆▆▆▆▆▆▆▆▅▆▅▆▆▅▅▅▅▄▄▅▄▅▅▄▄▃▃▂▃▂▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>9.76562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-meadow-3</strong> at: <a href='https://wandb.ai/dieplstks/BLIP-finetune/runs/5k39klsj' target=\"_blank\">https://wandb.ai/dieplstks/BLIP-finetune/runs/5k39klsj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231212_160402-5k39klsj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d6d97-e8d3-485b-8c64-7a5c4573634d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
